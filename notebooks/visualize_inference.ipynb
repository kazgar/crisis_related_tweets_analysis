{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from tweet_classification.constants import (\n",
    "    GRAPHS_PATH,\n",
    "    HUMAN_EXPERIMENT_NR,\n",
    "    INFO_EXPERIMENT_NR,\n",
    "    RESULTS_PATH,\n",
    ")\n",
    "from tweet_classification.utils import read_en_humanitarian_data as read_human_data\n",
    "from tweet_classification.utils import read_en_informativeness_data as read_info_data\n",
    "\n",
    "HUMAN_RESULTS_PATH = RESULTS_PATH / \"human_results\" / f\"exp_{HUMAN_EXPERIMENT_NR}\"\n",
    "HUMAN_GRAPHS_PATH = GRAPHS_PATH / \"human\" / f\"exp_{HUMAN_EXPERIMENT_NR}\"\n",
    "\n",
    "INFO_RESULTS_PATH = RESULTS_PATH / \"info_results\" / f\"exp_{INFO_EXPERIMENT_NR}\"\n",
    "INFO_GRAPHS_PATH = GRAPHS_PATH / \"info\" / f\"exp_{INFO_EXPERIMENT_NR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_comparison(df: pd.DataFrame, title: str, savepath: Path | None = None):\n",
    "    metrics = df.columns\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "\n",
    "    width = 0.4\n",
    "\n",
    "    paper_results = df.loc[df.index[0]].tolist()\n",
    "    bert_results = df.loc[df.index[1]].tolist()\n",
    "\n",
    "    fig, ax = plt.subplots(layout=\"constrained\", figsize=(10, 6))\n",
    "\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, linestyle=\"dotted\")\n",
    "\n",
    "    ax.bar(x - width / 2, paper_results, width=width, label=\"CNN Results (Paper)\", color=\"#4C72B0\")\n",
    "    ax.bar(\n",
    "        x + width / 2,\n",
    "        bert_results,\n",
    "        width=width,\n",
    "        label=\"Pretrained Bert + Fine-tuning\",\n",
    "        color=\"#55A868\",\n",
    "    )\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, padding=3, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Metric Value (%)\", fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_title(title, fontsize=18, fontweight=\"bold\", pad=15)\n",
    "    ax.legend(fontsize=16)\n",
    "    plt.ylim(min(paper_results + bert_results) - 0.05, max(bert_results + paper_results) + 0.05)\n",
    "    plt.show()\n",
    "\n",
    "    if savepath:\n",
    "        fig.savefig(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_human_perf = pd.DataFrame.from_dict(\n",
    "    {\"accuracy\": [0.835], \"precision\": [0.827], \"recall\": [0.840], \"f1\": [0.829]}\n",
    ")\n",
    "paper_human_perf.index = [\"cnn_paper_results\"]\n",
    "\n",
    "perf_columns = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "human_perf = pd.read_csv(HUMAN_RESULTS_PATH / \"performance_metrics.csv\")[perf_columns]\n",
    "for col in perf_columns:\n",
    "    human_perf[col] = human_perf[col].map(lambda x: round(x, 3))\n",
    "human_perf.index = [\"bert_fine_tuned_results\"]\n",
    "\n",
    "human_results_comparison = pd.concat([paper_human_perf, human_perf])\n",
    "print(human_results_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_comparison(\n",
    "    human_results_comparison,\n",
    "    title=\"Model Performance Comparison (Humanitarian)\",\n",
    "    savepath=HUMAN_GRAPHS_PATH / \"performance_comparison.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info_perf = pd.DataFrame.from_dict(\n",
    "    {\"accuracy\": [0.872], \"precision\": [0.866], \"recall\": [0.870], \"f1\": [0.866]}\n",
    ")\n",
    "\n",
    "paper_info_perf.index = [\"cnn_paper_results\"]\n",
    "\n",
    "info_perf = pd.read_csv(INFO_RESULTS_PATH / \"performance_metrics.csv\")[perf_columns]\n",
    "for col in perf_columns:\n",
    "    info_perf[col] = info_perf[col].map(lambda x: round(x, 3))\n",
    "info_perf.index = [\"bert_fine_tuned_results\"]\n",
    "\n",
    "info_results_comparison = pd.concat([paper_info_perf, info_perf])\n",
    "print(info_results_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance_comparison(\n",
    "    info_results_comparison,\n",
    "    title=\"Model Performance Comparison (Informativeness)\",\n",
    "    savepath=INFO_GRAPHS_PATH / \"performance_comparison.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Prediction Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_train_df, _, _ = read_human_data()\n",
    "\n",
    "human_labels_to_nrs = {\n",
    "    label: i for i, label in enumerate(human_train_df[\"class_label\"].unique().tolist())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_pred_vs_label = pd.read_csv(HUMAN_RESULTS_PATH / \"predictions.csv\")\n",
    "human_preds = human_pred_vs_label[\"predictions\"].tolist()\n",
    "human_actual = human_pred_vs_label[\"true_labels\"].tolist()\n",
    "\n",
    "human_confusion_matrix = metrics.confusion_matrix(human_actual, human_preds)\n",
    "human_labels = list(human_labels_to_nrs.keys())\n",
    "\n",
    "human_cm_display = metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=human_confusion_matrix, display_labels=human_labels\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "human_cm_display.plot(\n",
    "    ax=ax, cmap=\"Blues\", colorbar=True, values_format=\"d\", xticks_rotation=45, include_values=True\n",
    ")\n",
    "\n",
    "ax.set_xticks(np.arange(len(human_labels)))\n",
    "ax.set_yticks(np.arange(len(human_labels)))\n",
    "ax.set_xticklabels(human_labels, rotation=45, fontsize=13, ha=\"right\", fontweight=\"bold\")\n",
    "ax.set_yticklabels(human_labels, fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\"(Humanitarian) Predictions vs True Labels\", fontsize=20, fontweight=\"bold\", pad=20)\n",
    "plt.grid(False)\n",
    "plt.ylabel(\"True Label\", fontsize=20, fontweight=\"bold\")\n",
    "plt.xlabel(\"Predicted Label\", fontsize=20, fontweight=\"bold\")\n",
    "plt.savefig(HUMAN_GRAPHS_PATH / \"conf_matrix.png\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_train_df, _, _ = read_info_data()\n",
    "\n",
    "info_labels_to_nrs = {\n",
    "    label: i for i, label in enumerate(info_train_df[\"class_label\"].unique().tolist())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_pred_vs_label = pd.read_csv(INFO_RESULTS_PATH / \"predictions.csv\")\n",
    "info_preds = info_pred_vs_label[\"predictions\"].tolist()\n",
    "info_actual = info_pred_vs_label[\"true_labels\"].tolist()\n",
    "\n",
    "info_conf_matrix = metrics.confusion_matrix(info_actual, info_preds)\n",
    "info_cm_display = metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=info_conf_matrix, display_labels=[0, 1]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "info_cm_display.plot(ax=ax, cmap=\"Blues\", colorbar=True, values_format=\"d\")\n",
    "plt.title(\"(Informativeness) Predictions vs True Labels\", fontsize=20, fontweight=\"bold\", pad=20)\n",
    "plt.xticks(fontsize=15, fontweight=\"bold\")\n",
    "plt.yticks(fontsize=15, fontweight=\"bold\")\n",
    "plt.grid(False)\n",
    "plt.ylabel(\"True Label\", fontsize=20, fontweight=\"bold\", labelpad=15)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=20, fontweight=\"bold\", labelpad=15)\n",
    "plt.savefig(INFO_GRAPHS_PATH / \"conf_matrix.png\")\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
