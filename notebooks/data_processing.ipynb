{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re\n",
    "\n",
    "from constants import PROJECT_ROOT\n",
    "from utils import read_en_humanitarian_data as read_human_data\n",
    "from utils import read_en_informativeness_data as read_info_data\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"rt\\s+\", \"\", text)\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if text.startswith(\": \"):\n",
    "        text = text[2:]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Humanitarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_train_df, human_dev_df, human_test_df = read_human_data(dataset=\"all_data_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_train_df[\"text\"] = human_train_df[\"text\"].apply(clean_text)\n",
    "human_dev_df[\"text\"] = human_dev_df[\"text\"].apply(clean_text)\n",
    "human_test_df[\"text\"] = human_test_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Informativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_train_df, info_dev_df, info_test_df = read_info_data(dataset=\"all_data_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_train_df[\"text\"] = info_train_df[\"text\"].apply(clean_text)\n",
    "info_dev_df[\"text\"] = info_dev_df[\"text\"].apply(clean_text)\n",
    "info_test_df[\"text\"] = info_test_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Save Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "CLEAN_DATA_PATH = DATA_PATH / \"clean_en_data\"\n",
    "\n",
    "os.makedirs(CLEAN_DATA_PATH, exist_ok=True)\n",
    "\n",
    "for human_df, split in zip([human_train_df, human_dev_df, human_test_df], splits):\n",
    "    human_df.to_csv(CLEAN_DATA_PATH / f\"human_{split}.csv\", index=False)\n",
    "\n",
    "for info_df, split in zip([info_train_df, info_dev_df, info_test_df], splits):\n",
    "    info_df.to_csv(CLEAN_DATA_PATH / f\"info_{split}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
